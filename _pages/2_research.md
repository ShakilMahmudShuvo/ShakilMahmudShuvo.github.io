---
layout: page
permalink: /research/
title: Research
description: My research focuses on applying artificial intelligence and machine learning to solve critical challenges in healthcare, with particular emphasis on medical imaging, natural language processing for mental health, and computer vision applications.
nav: true
nav_order: 2
---

## Research Overview

I am a **Machine Learning Researcher** with **6 publications** (2 as first author) in the field of **AI for Healthcare**. My research spans multiple domains including **Medical Imaging & AI**, **NLP for Mental Health**, **Computer Vision**, and **Deep Learning Innovation**. I have consistently achieved state-of-the-art results in medical image analysis, with accuracies ranging from **96.33%** to **99.33%** across different healthcare applications.

My work has been published in reputable venues including **ICCA 2024**, **BIM 2023**, and **NCIM 2023**, demonstrating both theoretical contributions and practical implementations that address real-world healthcare challenges.

---

## Research Areas

<button class="btn btn-primary mb-3" type="button" data-toggle="collapse" data-target="#researchAreasCollapse" aria-expanded="false" aria-controls="researchAreasCollapse">
  <i class="fas fa-microscope mr-2"></i>View Research Areas Details
</button>

<div class="collapse" id="researchAreasCollapse">
  <div class="row">
    <div class="col-md-6 mb-4">
      <div class="card h-100 shadow-sm research-card">
        <div class="card-body">
          <div class="d-flex align-items-center mb-3">
            <i class="fas fa-brain text-primary fa-2x mr-3"></i>
            <h5 class="card-title mb-0">Medical Imaging & AI</h5>
          </div>
          <p class="card-text">Developing advanced deep learning architectures for medical image analysis, including brain tumor classification, glioma segmentation, and retinal disease detection.</p>
          <div class="mt-3">
            <span class="badge badge-primary mr-2">4 Publications</span>
            <span class="badge badge-success mr-2">96.33% - 99.33% Accuracy</span>
            <span class="badge badge-info">CNN, U-Net, ResNet</span>
          </div>
        </div>
      </div>
    </div>
    
    <div class="col-md-6 mb-4">
      <div class="card h-100 shadow-sm research-card">
        <div class="card-body">
          <div class="d-flex align-items-center mb-3">
            <i class="fas fa-comments text-success fa-2x mr-3"></i>
            <h5 class="card-title mb-0">NLP for Mental Health</h5>
          </div>
          <p class="card-text">Applying natural language processing techniques to detect early signs of mental health issues, particularly suicidal ideation detection from social media text.</p>
          <div class="mt-3">
            <span class="badge badge-primary mr-2">1 Publication</span>
            <span class="badge badge-success mr-2">96.33% Accuracy</span>
            <span class="badge badge-info">BERT, Transformers</span>
          </div>
        </div>
      </div>
    </div>
    
    <div class="col-md-6 mb-4">
      <div class="card h-100 shadow-sm research-card">
        <div class="card-body">
          <div class="d-flex align-items-center mb-3">
            <i class="fas fa-eye text-warning fa-2x mr-3"></i>
            <h5 class="card-title mb-0">Computer Vision</h5>
          </div>
          <p class="card-text">Developing robust computer vision systems for medical applications, including retinal OCT analysis and advanced image preprocessing techniques.</p>
          <div class="mt-3">
            <span class="badge badge-primary mr-2">Active Research</span>
            <span class="badge badge-info">OpenCV, CNN, Image Processing</span>
          </div>
        </div>
      </div>
    </div>
    
    <div class="col-md-6 mb-4">
      <div class="card h-100 shadow-sm research-card">
        <div class="card-body">
          <div class="d-flex align-items-center mb-3">
            <i class="fas fa-network-wired text-danger fa-2x mr-3"></i>
            <h5 class="card-title mb-0">Deep Learning Innovation</h5>
          </div>
          <p class="card-text">Creating novel neural network architectures and fusion techniques to improve performance in healthcare applications, including late fusion CNN approaches.</p>
          <div class="mt-3">
            <span class="badge badge-primary mr-2">1 Publication</span>
            <span class="badge badge-success mr-2">99.33% Accuracy</span>
            <span class="badge badge-info">CNN Fusion, Novel Architectures</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

---

## Research Impact & Metrics

<div class="row text-center mb-4">
  <div class="col-md-2 col-6 mb-3">
    <div class="metric-card">
      <i class="fas fa-file-alt text-primary fa-2x mb-2"></i>
      <h4 class="metric-number">{{ site.data.scholar.publications | default: "6" }}</h4>
      <p class="metric-label">Publications</p>
    </div>
  </div>
  <div class="col-md-2 col-6 mb-3">
    <div class="metric-card">
      <i class="fas fa-quote-left text-success fa-2x mb-2"></i>
      <h4 class="metric-number">{{ site.data.scholar.citations | default: "8" }}</h4>
      <p class="metric-label">Citations</p>
    </div>
  </div>
  <div class="col-md-2 col-6 mb-3">
    <div class="metric-card">
      <i class="fas fa-chart-line text-warning fa-2x mb-2"></i>
      <h4 class="metric-number">{{ site.data.scholar.h_index | default: "2" }}</h4>
      <p class="metric-label">h-index</p>
    </div>
  </div>
  <div class="col-md-2 col-6 mb-3">
    <div class="metric-card">
      <i class="fas fa-microscope text-info fa-2x mb-2"></i>
      <h4 class="metric-number">4</h4>
      <p class="metric-label">Research Areas</p>
    </div>
  </div>
  <div class="col-md-2 col-6 mb-3">
    <div class="metric-card">
      <i class="fas fa-percentage text-danger fa-2x mb-2"></i>
      <h4 class="metric-number">98.2%</h4>
      <p class="metric-label">Avg. Accuracy</p>
    </div>
  </div>
  <div class="col-md-2 col-6 mb-3">
    <div class="metric-card">
      <i class="fas fa-star text-purple fa-2x mb-2"></i>
      <h4 class="metric-number">2</h4>
      <p class="metric-label">First Author</p>
    </div>
  </div>
</div>

<div class="text-center mb-4">
    <a href="https://scholar.google.com/citations?user=EOsiW3sAAAAJ&hl=en&authuser=3" 
       target="_blank" 
       class="btn btn-outline-primary btn-lg">
        <i class="ai ai-google-scholar ai-lg mr-2"></i>
        <strong>View Complete Google Scholar Profile</strong>
        <i class="fas fa-external-link-alt ml-2"></i>
    </a>
</div>

---

## Current Research Interests

<button class="btn btn-outline-secondary mb-3" type="button" data-toggle="collapse" data-target="#futureResearchCollapse" aria-expanded="false" aria-controls="futureResearchCollapse">
  <i class="fas fa-rocket mr-2"></i>View Future Research Directions
</button>

<div class="collapse" id="futureResearchCollapse">
  <div class="row">
    <div class="col-md-6 mb-3">
      <div class="card border-light">
        <div class="card-body">
          <h6 class="card-title text-dark"><i class="fas fa-layer-group mr-2 text-muted"></i>Multimodal Learning</h6>
          <p class="card-text small text-muted">Integrating multiple data modalities (text, image, audio) for comprehensive healthcare analysis and diagnosis.</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 mb-3">
      <div class="card border-light">
        <div class="card-body">
          <h6 class="card-title text-dark"><i class="fas fa-network-wired mr-2 text-muted"></i>Federated Learning</h6>
          <p class="card-text small text-muted">Developing privacy-preserving machine learning systems for healthcare institutions while maintaining data security.</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 mb-3">
      <div class="card border-light">
        <div class="card-body">
          <h6 class="card-title text-dark"><i class="fas fa-search mr-2 text-muted"></i>Explainable AI</h6>
          <p class="card-text small text-muted">Creating interpretable machine learning models for healthcare applications to enhance trust and clinical adoption.</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 mb-3">
      <div class="card border-light">
        <div class="card-body">
          <h6 class="card-title text-dark"><i class="fas fa-shield-alt mr-2 text-muted"></i>Computer Security</h6>
          <p class="card-text small text-muted">Exploring adversarial attacks and defenses in medical AI systems, ensuring robustness against malicious inputs.</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 mb-3">
      <div class="card border-light">
        <div class="card-body">
          <h6 class="card-title text-dark"><i class="fas fa-robot mr-2 text-muted"></i>Large Language Models</h6>
          <p class="card-text small text-muted">Adapting and fine-tuning LLMs for healthcare applications, including medical question answering and clinical decision support.</p>
        </div>
      </div>
    </div>
    <div class="col-md-6 mb-3">
      <div class="card border-light">
        <div class="card-body">
          <h6 class="card-title text-dark"><i class="fas fa-heartbeat mr-2 text-muted"></i>AI for Global Health</h6>
          <p class="card-text small text-muted">Developing scalable AI solutions for healthcare challenges in resource-limited settings and underserved populations.</p>
        </div>
      </div>
    </div>
  </div>
</div>

---

## Publications

A complete list of my publications is provided below. My **Google Scholar** profile is available [here](https://scholar.google.com/citations?user=EOsiW3sAAAAJ&hl=en&authuser=3). These publications demonstrate my expertise in developing novel deep learning architectures for healthcare applications, with consistent focus on achieving state-of-the-art performance while addressing real-world challenges.

<!-- Publications Section -->
<div class="publications">

<h2>2024</h2>

<ol class="bibliography">

<li>
<div class="row">
  <div class="col-sm-3 preview">
    <img src="/assets/img/publication_preview/suicidal_ideation.png" class="preview z-depth-1 rounded" alt="suicidal ideation preview">
  </div>
  <div class="col-sm-9">
    <div class="title">
      <a href="https://dl.acm.org/doi/10.1145/3723178.3723242" target="_blank">Early Detection of Suicidal Ideation Using Bidirectional GRU and Language Models</a>
    </div>
    <div class="author">
      <strong>Shakil Mahmud Shuvo</strong>, Navia Novely, Md. Farukuzzaman Faruk, Azmain Yakin Srizon, S. M. Mahedy Hasan
    </div>
    <div class="periodical">
      <em>Proceedings of the <a href="https://icca.acm.org/" target="_blank">3rd International Conference on Computing Advancements (ICCA)</a></em>, 2024
    </div>
    <div class="note"><span class="badge badge-danger">First Author</span></div>
    
    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    <a href="https://dl.acm.org/doi/10.1145/3723178.3723242" class="btn btn-sm z-depth-0" role="button" target="_blank">Paper</a>
    
    <div class="abstract hidden">
      <p>Suicide has recently emerged as a leading cause of death worldwide, underlining the importance of effective preventative measures. Online social media posts can provide valuable insights into people who are suicidal and assist in preventing unfortunate outcomes. However, the problem lies in the complexities of such posts, as people may be hesitant to share their distressing thoughts due to different psychological and societal constraints. This study examined the utilization of Bidirectional GRU, a type of recurrent neural network that analyzes data sequences in both forward and backward directions, to improve text classification using language models. This study improves textual analysis by incorporating Bidirectional GRU (Bi-GRU) layers with popular pre-trained language models like BERT, RoBERTa, DistilBERT, DistilRoBERTa, and ELECTRA-Small. The dataset used for this research is sourced from Reddit. The BERT-BiGRU and DistilBERT-BiGRU models demonstrated notable effectiveness, achieving accuracies of 95.8% and 95.2% respectively. These models also exhibited remarkably low false negative rates, with BERT-BiGRU at 4.17% and DistilBERT-BiGRU at 2.80%. These findings show that the combination of Bi-GRU and pre-trained language models considerably improves early detection of suicidal ideation, paving the path for prompt and perhaps lifesaving interventions.</p>
    </div>
  </div>
</div>
</li>

<li>
<div class="row">
  <div class="col-sm-3 preview">
    <img src="/assets/img/publication_preview/retinal_oct.png" class="preview z-depth-1 rounded" alt="retinal OCT preview">
  </div>
  <div class="col-sm-9">
    <div class="title">
      <a href="https://dl.acm.org/doi/10.1145/3723178.3723304" target="_blank">Improving Pre-Trained CNNs with CBAM and Skip Connections for Multi-Class Retinal Diseases Classification using OCT Images</a>
    </div>
    <div class="author">
      Navia Novely, <strong>Shakil Mahmud Shuvo</strong>, Md. Farukuzzaman Faruk
    </div>
    <div class="periodical">
      <em>Proceedings of the <a href="https://icca.acm.org/" target="_blank">3rd International Conference on Computing Advancements (ICCA)</a></em>, 2024
    </div>
    <div class="note"><span class="badge badge-info">Second Author</span></div>
    
    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    <a href="https://dl.acm.org/doi/10.1145/3723178.3723304" class="btn btn-sm z-depth-0" role="button" target="_blank">Paper</a>
    
    <div class="abstract hidden">
      <p>Millions of people suffer from retinal defects over the world. Early discovery and treatment of these anomalies could halt further progression, saving many people from preventable blindness. Disease detection through manual methods is a tedious procedure that is both time-consuming and cannot produce consistent results. Building on the groundwork laid by Deep Convolutional Neural Networks (DCNNs) in computer-aided diagnosis (CAD), there have been initiatives to automate the detection of retinal diseases. This study presented a hybrid framework that utilizes pre-trained models (DenseNet121, ResNet50, VGG16, Xception, and EfficientB1) incorporating the Convolutional Block Attention Module (CBAM) and skip connections for accurate retinal disease classification. The CBAM block improved retinal disease classification by focusing on important features in OCT images. Skip connections were implemented to facilitate the direct transfer of feature information between layers. This study utilized the OCT-C8 dataset, which comprises seven disease classes and one healthy class. The DenseNet-CBAM-Skip and Xception-CBAM-Skip architectures were particularly noteworthy for their outstanding performance, with DenseNet obtaining a high accuracy of 96.28% and Xception 96.11%. They also acquired a significant F1-score of 96.31% and 96.11% respectively, making it promising for real-time application to help ophthalmologists.</p>
    </div>
  </div>
</div>
</li>

<li>
<div class="row">
  <div class="col-sm-3 preview">
    <img src="/assets/img/publication_preview/glioma_segmentation.png" class="preview z-depth-1 rounded" alt="glioma segmentation preview">
  </div>
  <div class="col-sm-9">
    <div class="title">
      <a href="https://dl.acm.org/doi/10.1145/3723178.3723308" target="_blank">Advancing Glioma Segmentation: A Robust 3D Residual Attention U-Net Framework for Multimodal MRI Images</a>
    </div>
    <div class="author">
      Soumit Das, Md. Farukuzzaman Faruk, <strong>Shakil Mahmud Shuvo</strong>, Azmain Yakin Srizon, S. M. Mahedy Hasan, Prof. Dr. Md. Al Mamun
    </div>
    <div class="periodical">
      <em>Proceedings of the <a href="https://icca.acm.org/" target="_blank">3rd International Conference on Computing Advancements (ICCA)</a></em>, 2024
    </div>
    <div class="note"><span class="badge badge-secondary">Third Author</span></div>
    
    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    <a href="https://dl.acm.org/doi/10.1145/3723178.3723308" class="btn btn-sm z-depth-0" role="button" target="_blank">Paper</a>
    
    <div class="abstract hidden">
      <p>Brain tumors are abnormal growths of cells within the brain, posing significant health challenges. Among the different types of brain tumors, glioma, which originates from supportive glial tissue, is notably concerning due to its low survival rate compared to others. The prognosis of glioma hinges on various factors, including its location, size, and degree of extension. Therefore, accurate segmentation of glioma is crucial for both diagnosis and treatment planning. Manual segmentation by radiologists or specialized clinicians is prone to variability and is time-intensive. To automate tumor segmentation, a '3D Residual Attention U-Net' architecture is presented in this study. By integrating spatial and channel attention mechanisms, the model enhanced the feature representation. The "Vanishing Gradient" phenomenon in conventional 'U-Net' was solved using a series of residual directed blocks. Moreover, the use of a modified loss function called 'Focal-Dice' helped alleviate challenges associated with noisy labels and class imbalance in pixel segmentation tasks. The proposed architecture achieves a Dice coefficient of 0.9002 and an Intersection over Union (IoU) metric of 0.8272, demonstrating its efficacy in segmenting brain tumors. These results are obtained within a short training time, showcasing the model's efficiency. Overall, this research contributes to streamlining medical image analysis processes, ultimately enhancing patient care outcomes in glioma diagnosis and treatment.</p>
    </div>
  </div>
</div>
</li>

</ol>

<h2>2023</h2>

<ol class="bibliography">



<li>
<div class="row">
  <div class="col-sm-3 preview">
    <img src="/assets/img/publication_preview/brain_tumor.png" class="preview z-depth-1 rounded" alt="brain tumor preview">
  </div>
  <div class="col-sm-9">
    <div class="title">
      <a href="https://link.springer.com/chapter/10.1007/978-981-99-8937-9_37" target="_blank">Multi-class Brain Tumor Classification with DenseNet-Based Deep Learning Features and Ensemble of Machine Learning Approaches</a>
    </div>
    <div class="author">
      <strong>Shakil Mahmud Shuvo</strong>, Md. Farukuzzaman Faruk, Azmain Yakin Srizon, Tahsen Islam Sajon, S. M. Mahedy Hasan, Anirban Barai, A. F. M. Minhazur Rahman, Md. Al Mamun
    </div>
    <div class="periodical">
      <em>Proceedings of the <a href="https://link.springer.com/conference/bim" target="_blank">2nd International Conference on Big Data, IoT and Machine Learning (BIM)</a></em>, 2023
    </div>
    <div class="note"><span class="badge badge-danger">First Author</span></div>
    
    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    <a href="https://link.springer.com/chapter/10.1007/978-981-99-8937-9_37" class="btn btn-sm z-depth-0" role="button" target="_blank">Paper</a>
    
    <div class="abstract hidden">
      <p>The timely and precise diagnosis of brain tumors is crucial in reducing mortality rates. Although Magnetic Resonance Imaging (MRI) is commonly used as a detection tool for brain tumors, the presence of unwanted regions in MRI and multi-class brain MRI datasets may pose challenges in accurately classifying tumors. This study proposed a two-phase end-to-end framework comprising DenseNet-121-based deep learning to extract features and an ensemble of machine learning methodologies for precise classification. The deep learning-based feature extraction phase effectively extracted essential and discriminant features that were utilized by multiple machine learning models. Preprocessing MRI images to eliminate unwanted regions enhanced the deep learning model's feature extraction capabilities. The effectiveness of the proposed framework was evaluated by measuring the classification performance of the ensemble mechanism, which achieved an accuracy of 98.86% and an f1-score of 98.76% without any data augmentation. Notably, the random forest attained the utmost accuracy and f1-score among the machine learning approaches used in the ensemble technique.</p>
    </div>
  </div>
</div>
</li>

<li>
<div class="row">
  <div class="col-sm-3 preview">
    <img src="/assets/img/publication_preview/brain_tumor_mri.png" class="preview z-depth-1 rounded" alt="brain tumor MRI preview">
  </div>
  <div class="col-sm-9">
    <div class="title">
      <a href="https://ieeexplore.ieee.org/document/10212729" target="_blank">A Late Fusion Deep CNN Model for the Classification of Brain Tumors from Multi-Parametric MRI Images</a>
    </div>
    <div class="author">
      Anirban Barai, Md. Farukuzzaman Faruk, <strong>Shakil Mahmud Shuvo</strong>, Azmain Yakin Srizon, S. M. Mahedy Hasan, Abu Sayeed
    </div>
    <div class="periodical">
      <em>Proceedings of the <a href="https://ieeexplore.ieee.org/xpl/conhome/10212658/proceeding" target="_blank">2023 International Conference on Next-Generation Computing, IoT and Machine Learning (NCIM)</a></em>, 2023
    </div>
    <div class="note"><span class="badge badge-secondary">Third Author</span></div>
    
    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    <a href="https://ieeexplore.ieee.org/document/10212729" class="btn btn-sm z-depth-0" role="button" target="_blank">Paper</a>
    
    <div class="abstract hidden">
      <p>Precise classification of brain tumors is crucial for legitimate clinical diagnosis, prognosis and treatment decisions. Multi-parametric magnetic resonance imaging (MRI) is an intriguing option for improving brain tumor classification accuracy. However, combining information from different MRI sequences poses a challenge, as each sequence provides unique and complementary information about the tumor. To overcome this challenge, this study proposed a late fusion CNN architecture that integrates features extracted from each MRI sequence at a later stage in the classification process. This approach facilitates the model to capture the unique features of each MRI sequence while also leveraging the supplemental data extracted from the other sequences. This study presents the implementation and evaluation of a cutting-edge deep learning-based late fusion multi-parametric brain tumor classification approach. The overall findings, alongside the feature maps, demonstrate how this strategy has the potential to enhance brain tumor classification accuracy and provide valuable insights for clinical decision-making. This investigation achieved 97% test accuracy.</p>
    </div>
  </div>
</li>



<li>
<div class="row">
  <div class="col-sm-3 preview">
    <img src="/assets/img/publication_preview/leukemia_classification.png" class="preview z-depth-1 rounded" alt="leukemia classification preview">
  </div>
  <div class="col-sm-9">
    <div class="title">
      <a href="https://link.springer.com/chapter/10.1007/978-981-99-8937-9_24" target="_blank">Attention Mechanism-Enhanced Deep CNN Architecture for Precise Multi-class Leukemia Classification</a>
    </div>
    <div class="author">
      Tahsen Islam Sajon, Barsha Roy, Md. Farukuzzaman Faruk, Azmain Yakin Srizon, <strong>Shakil Mahmud Shuvo</strong>, Md. Al Mamun, Abu Sayeed, S. M. Mahedy Hasan
    </div>
    <div class="periodical">
      <em>Proceedings of the <a href="https://link.springer.com/conference/bim" target="_blank">2nd International Conference on Big Data, IoT and Machine Learning (BIM)</a></em>, 2023
    </div>
    <div class="note"><span class="badge badge-secondary">Fifth Author</span></div>
    
    <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    <a href="https://link.springer.com/chapter/10.1007/978-981-99-8937-9_24" class="btn btn-sm z-depth-0" role="button" target="_blank">Paper</a>
    
    <div class="abstract hidden">
      <p>Leukemia is a life-threatening condition affecting people globally, making accurate diagnosis crucial for timely intervention. Consequently, researchers have been exploring automated methods to enable prompt action. The classification of leukemia into multiple subtypes according to WHO standards presents a unique challenge. Unlike binary classification, interclass features are highly similar, leading to misclassification. Ergo, we employ attention mechanisms to tackle this problem. Our proposed deep learning architecture combines transfer learning with attention mechanisms to classify subtypes of leukemia accurately. Using a publicly available dataset of blood cell images that adhered to WHO standards, we illustrate the potency of our approach. Our DenseNet201 with CBAM model achieves a remarkable 99.85% overall accuracy without resorting to data augmentation, surpassing previous methods on this dataset and attaining state-of-the-art results compared to other leukemia literature. To interpret the model's decision-making process and evaluate the efficacy of the attention mechanism in identifying discriminating features, we showcase GradCAM images and intermediate layer feature maps generated from our custom CNN. The proposed approach enhances leukemia classification accuracy and efficiency, providing clinical decision-making insights.</p>
    </div>
  </div>
</div>
</li>

</ol>

</div>

<style>
/* Research Cards Hover Effects */
.research-card {
    transition: all 0.3s ease;
    border: none;
}

.research-card:hover {
    transform: translateY(-5px);
    box-shadow: 0 10px 30px rgba(0,0,0,0.1) !important;
}

/* Metric Cards */
.metric-card {
    padding: 20px 10px;
    border-radius: 8px;
    background: #f8f9fa;
    transition: all 0.3s ease;
    border: 1px solid #e9ecef;
}

.metric-card:hover {
    transform: translateY(-2px);
    border-color: #dee2e6;
    background: #ffffff;
    box-shadow: 0 4px 12px rgba(0,0,0,0.05);
}

.metric-number {
    font-weight: bold;
    margin-bottom: 5px;
    color: #333;
}

.metric-label {
    margin-bottom: 0;
    color: #666;
    font-size: 0.9rem;
}

/* Purple color for custom elements */
.text-purple {
    color: #6f42c1 !important;
}

.border-purple {
    border-color: #6f42c1 !important;
}

/* Bibliography Styling */
.publications ol.bibliography {
    list-style: none;
    padding-left: 0;
    margin-top: 20px;
}

.publications ol.bibliography li {
    margin-bottom: 30px;
    padding: 20px 0;
    background: transparent;
    border-radius: 0;
    transition: all 0.3s ease;
}

.publications ol.bibliography li:hover {
    background: transparent;
    transform: translateY(-2px);
    box-shadow: none;
}

.publications .title {
    font-weight: 600;
    font-size: 1.1rem;
    margin-bottom: 5px;
    color: #333;
}

.publications .author {
    color: #666;
    margin-bottom: 5px;
}

.publications .periodical {
    color: #888;
    font-size: 0.95rem;
    margin-bottom: 10px;
}

.publications .note {
    margin-bottom: 10px;
}

.publications .links {
    margin-top: 10px;
}

.publications .abbr {
    text-align: center;
    display: flex;
    align-items: center;
    justify-content: center;
    height: 100%;
}

/* Elegant Year Division Styling */
.publications h2,
.bibliography h2 {
    position: relative !important;
    text-align: left !important;
    margin: 50px 0 30px 0 !important;
    padding: 15px 0 10px 0 !important;
    background: none !important;
    color: var(--global-text-color) !important;
    border-bottom: 2px solid #e9ecef !important;
    font-size: 1.8rem !important;
    font-weight: 600 !important;
    letter-spacing: 0.5px !important;
    transition: all 0.3s ease !important;
    box-shadow: none !important;
    text-shadow: none !important;
    border-radius: 0 !important;
    overflow: visible !important;
}

.publications h2:hover,
.bibliography h2:hover {
    color: #212529 !important;
    border-bottom-color: #adb5bd !important;
    transform: none !important;
    box-shadow: none !important;
}

/* Clean underline animation */
.publications h2:before,
.bibliography h2:before {
    content: '' !important;
    position: absolute !important;
    bottom: -2px !important;
    left: 0 !important;
    width: 0 !important;
    height: 2px !important;
    background: #6c757d !important;
    transition: width 0.3s ease !important;
}

.publications h2:hover:before,
.bibliography h2:hover:before {
    width: 60px !important;
}

/* Remove calendar icon */
.publications h2:after,
.bibliography h2:after {
    display: none !important;
}

/* Thumbnail styling */
.publications .preview {
    text-align: center;
    display: flex;
    align-items: center;
    justify-content: center;
    padding: 0 15px;
}

.publications .preview img {
    max-width: 180px;
    width: 100%;
    height: auto;
    box-shadow: 0 3px 10px rgba(0,0,0,0.1);
    transition: all 0.3s ease;
}

.publications .preview img:hover {
    transform: scale(1.05);
    box-shadow: 0 5px 20px rgba(0,0,0,0.2);
}

/* Minimal button styling */
.publications .abstract.btn {
    background: transparent;
    border: 1px solid #dee2e6;
    color: #495057;
    padding: 0.25rem 0.75rem;
    margin-right: 0.5rem;
    transition: all 0.2s ease;
    font-size: 0.875rem;
    font-weight: 400;
}

.publications .abstract.btn:hover {
    background: #f8f9fa;
    border-color: #adb5bd;
    color: #212529;
    text-decoration: none;
}

/* Paper button styling - minimal */
.publications a.btn[href*="doi"],
.publications a.btn[href*="ieee"],
.publications a.btn[href*="springer"],
.publications a.btn[href*="acm"] {
    background: transparent;
    border: 1px solid #dee2e6;
    color: #495057;
    padding: 0.25rem 0.75rem;
    transition: all 0.2s ease;
    font-size: 0.875rem;
    font-weight: 400;
}

.publications a.btn[href*="doi"]:hover,
.publications a.btn[href*="ieee"]:hover,
.publications a.btn[href*="springer"]:hover,
.publications a.btn[href*="acm"]:hover {
    background: #f8f9fa;
    border-color: #adb5bd;
    color: #212529;
    text-decoration: none;
}

/* Badge styling - minimal */
.publications .badge {
    background-color: #f8f9fa !important;
    color: #495057 !important;
    border: 1px solid #dee2e6 !important;
    font-weight: 400 !important;
    padding: 0.35em 0.65em !important;
}

.publications .badge-danger,
.publications .badge-info,
.publications .badge-secondary {
    background-color: #f8f9fa !important;
    color: #495057 !important;
}

/* Conference links styling */
.publications .periodical a {
    color: #495057;
    text-decoration: none;
    border-bottom: 1px dotted #495057;
    transition: all 0.2s ease;
}

.publications .periodical a:hover {
    color: #495057;
    border-bottom-color: #212529;
}

/* Abstract content styling */
.publications .abstract.hidden {
    display: none;
}

.publications .abstract:not(.btn) {
    margin-top: 15px;
    padding: 15px 0 15px 20px;
    background: transparent;
    border-left: 2px solid #dee2e6;
    border-radius: 0;
    font-size: 0.95rem;
    line-height: 1.6;
    color: #6c757d;
}

/* Hide any stray HTML tags */
.publications ol.bibliography:after {
    content: '';
    display: none;
}

/* Responsive adjustments */
@media (max-width: 768px) {
    .metric-card {
        margin-bottom: 15px;
    }
    
    .publications h2,
    .bibliography h2 {
        font-size: 1.6rem !important;
        margin: 40px 0 25px 0 !important;
    }
}

@media (max-width: 576px) {
    .publications h2,
    .bibliography h2 {
        font-size: 1.4rem !important;
        margin: 35px 0 20px 0 !important;
    }
}
</style>

<script>
// Abstract toggle functionality
document.addEventListener('DOMContentLoaded', function() {
    const abstractButtons = document.querySelectorAll('.publications .abstract.btn');
    
    abstractButtons.forEach(button => {
        button.addEventListener('click', function() {
            const abstractDiv = this.parentElement.querySelector('.abstract:not(.btn)');
            if (abstractDiv) {
                abstractDiv.classList.toggle('hidden');
                this.textContent = abstractDiv.classList.contains('hidden') ? 'Abstract' : 'Hide Abstract';
            }
        });
    });
});
</script>
