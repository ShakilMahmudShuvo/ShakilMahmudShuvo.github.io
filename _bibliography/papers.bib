---
---


@inproceedings{shuvo_suicidal_2024,
  author = {Shuvo, Shakil Mahmud and Novely, Navia and Faruk, Md. Farukuzzaman and Srizon, Azmain Yakin and Hasan, S. M. Mahedy},
  title = {Early Detection of Suicidal Ideation Using Bidirectional GRU and Language Models},
  booktitle = {Proceedings of the <a href="https://icca.aiub.edu/">3rd International Conference on Computing Advancements</a>},
  year = {2024},
  pages = {482--490},
  note = {First Author},
  selected = {true},
  url = {https://dl.acm.org/doi/10.1145/3723178.3723242},
  abstract = {Suicide has recently emerged as a leading cause of death worldwide, underlining the importance of effective preventative measures. Online social media posts can provide valuable insights into people who are suicidal and assist in preventing unfortunate outcomes. However, the problem lies in the complexities of such posts, as people may be hesitant to share their distressing thoughts due to different psychological and societal constraints. This study examined the utilization of Bidirectional GRU, a type of recurrent neural network that analyzes data sequences in both forward and backward directions, to improve text classification using language models. This study improves textual analysis by incorporating Bidirectional GRU (Bi-GRU) layers with popular pre-trained language models like BERT, RoBERTa, DistilBERT, DistilRoBERTa, and ELECTRA-Small. The dataset used for this research is sourced from Reddit. The BERT-BiGRU and DistilBERT-BiGRU models demonstrated notable effectiveness, achieving accuracies of 95.8% and 95.2% respectively. These models also exhibited remarkably low false negative rates, with BERT-BiGRU at 4.17% and DistilBERT-BiGRU at 2.80%. These findings show that the combination of Bi-GRU and pre-trained language models considerably improves early detection of suicidal ideation, paving the path for prompt and perhaps lifesaving interventions.},
  preview = {suicidal_ideation.png}
}

@inproceedings{shuvo_brain_tumor_2023,
  author = {Shuvo, Shakil Mahmud and Faruk, Md. Farukuzzaman and Srizon, Azmain Yakin and Sajon, Tahsen Islam and Hasan, S. M. Mahedy and Barai, Anirban and Rahman, A. F. M. Minhazur and Mamun, Md. Al},
  title = {Multi-class Brain Tumor Classification with DenseNet-Based Deep Learning Features and Ensemble of Machine Learning Approaches},
  booktitle = {Proceedings of the <a href="https://confbim.com/">2nd International Conference on Big Data, IoT and Machine Learning</a>},
  year = {2023},
  pages = {559--573},
  note = {First Author},
  url = {https://link.springer.com/chapter/10.1007/978-981-99-8937-9_37},
  abstract = {The timely and precise diagnosis of brain tumors is crucial in reducing mortality rates. Although Magnetic Resonance Imaging (MRI) is commonly used as a detection tool for brain tumors, the presence of unwanted regions in MRI and multi-class brain MRI datasets may pose challenges in accurately classifying tumors. This study proposed a two-phase end-to-end framework comprising DenseNet-121-based deep learning to extract features and an ensemble of machine learning methodologies for precise classification. The deep learning-based feature extraction phase effectively extracted essential and discriminant features that were utilized by multiple machine learning models. Preprocessing MRI images to eliminate unwanted regions enhanced the deep learning model's feature extraction capabilities. The effectiveness of the proposed framework was evaluated by measuring the classification performance of the ensemble mechanism, which achieved an accuracy of 98.86% and an f1-score of 98.76% without any data augmentation. Notably, the random forest attained the utmost accuracy and f1-score among the machine learning approaches used in the ensemble technique.},
  preview = {brain_tumor.png}
}

@inproceedings{novely_retinal_2024,
  author = {Novely, Navia and Shuvo, Shakil Mahmud and Faruk, Md. Farukuzzaman},
  title = {Improving Pre-Trained CNNs with CBAM and Skip Connections for Multi-Class Retinal Diseases Classification using OCT Images},
  booktitle = {Proceedings of the <a href="https://icca.aiub.edu/">3rd International Conference on Computing Advancements</a>},
  year = {2024},
  pages = {946--953},
  note = {Second Author},
  selected = {true},
  url = {https://dl.acm.org/doi/10.1145/3723178.3723304},
  abstract = {Millions of people suffer from retinal defects over the world. Early discovery and treatment of these anomalies could halt further progression, saving many people from preventable blindness. Disease detection through manual methods is a tedious procedure that is both time-consuming and cannot produce consistent results. Building on the groundwork laid by Deep Convolutional Neural Networks (DCNNs) in computer-aided diagnosis (CAD), there have been initiatives to automate the detection of retinal diseases. This study presented a hybrid framework that utilizes pre-trained models (DenseNet121, ResNet50, VGG16, Xception, and EfficientB1) incorporating the Convolutional Block Attention Module (CBAM) and skip connections for accurate retinal disease classification. The CBAM block improved retinal disease classification by focusing on important features in OCT images. Skip connections were implemented to facilitate the direct transfer of feature information between layers. This study utilized the OCT-C8 dataset, which comprises seven disease classes and one healthy class. The DenseNet-CBAM-Skip and Xception-CBAM-Skip architectures were particularly noteworthy for their outstanding performance, with DenseNet obtaining a high accuracy of 96.28% and Xception 96.11%. They also acquired a significant F1-score of 96.31% and 96.11% respectively, making it promising for real-time application to help ophthalmologists.},
  preview = {retinal_oct.png}
}

@inproceedings{das_glioma_2024,
  author = {Das, Soumit and Faruk, Md. Farukuzzaman and Shuvo, Shakil Mahmud and Srizon, Azmain Yakin and Hasan, S. M. Mahedy and Mamun, Prof. Dr. Md. Al},
  title = {Advancing Glioma Segmentation: A Robust 3D Residual Attention U-Net Framework for Multimodal MRI Images},
  booktitle = {Proceedings of the <a href="https://icca.aiub.edu/">3rd International Conference on Computing Advancements</a>},
  year = {2024},
  pages = {978--985},
  note = {Third Author},
  url = {https://dl.acm.org/doi/10.1145/3723178.3723308},
  abstract = {Brain tumors are abnormal growths of cells within the brain, posing significant health challenges. Among the different types of brain tumors, glioma, which originates from supportive glial tissue, is notably concerning due to its low survival rate compared to others. The prognosis of glioma hinges on various factors, including its location, size, and degree of extension. Therefore, accurate segmentation of glioma is crucial for both diagnosis and treatment planning. Manual segmentation by radiologists or specialized clinicians is prone to variability and is time-intensive. To automate tumor segmentation, a '3D Residual Attention U-Net' architecture is presented in this study. By integrating spatial and channel attention mechanisms, the model enhanced the feature representation. The "Vanishing Gradient" phenomenon in conventional 'U-Net' was solved using a series of residual directed blocks. Moreover, the use of a modified loss function called 'Focal-Dice' helped alleviate challenges associated with noisy labels and class imbalance in pixel segmentation tasks. The proposed architecture achieves a Dice coefficient of 0.9002 and an Intersection over Union (IoU) metric of 0.8272, demonstrating its efficacy in segmenting brain tumors. These results are obtained within a short training time, showcasing the model's efficiency. Overall, this research contributes to streamlining medical image analysis processes, ultimately enhancing patient care outcomes in glioma diagnosis and treatment.},
  preview = {glioma_segmentation.png}
}

@inproceedings{barai_brain_tumor_2023,
  author = {Barai, Anirban and Faruk, Md. Farukuzzaman and Shuvo, Shakil Mahmud and Srizon, Azmain Yakin and Hasan, S. M. Mahedy and Sayeed, Abu},
  title = {A Late Fusion Deep CNN Model for the Classification of Brain Tumors from Multi-Parametric MRI Images},
  booktitle = {Proceedings of the <a href="https://ncim2023.cse.duet.ac.bd/">2023 International Conference on Next-Generation Computing, IoT and Machine Learning</a>},
  year = {2023},
  pages = {1--6},
  note = {Third Author},
  url = {https://ieeexplore.ieee.org/document/10212729},
  abstract = {Precise classification of brain tumors is crucial for legitimate clinical diagnosis, prognosis and treatment decisions. Multi-parametric magnetic resonance imaging (MRI) is an intriguing option for improving brain tumor classification accuracy. However, combining information from different MRI sequences poses a challenge, as each sequence provides unique and complementary information about the tumor. To overcome this challenge, this study proposed a late fusion CNN architecture that integrates features extracted from each MRI sequence at a later stage in the classification process. This approach facilitates the model to capture the unique features of each MRI sequence while also leveraging the supplemental data extracted from the other sequences. This study presents the implementation and evaluation of a cutting-edge deep learning-based late fusion multi-parametric brain tumor classification approach. The overall findings, alongside the feature maps, demonstrate how this strategy has the potential to enhance brain tumor classification accuracy and provide valuable insights for clinical decision-making. This investigation achieved 97% test accuracy.},
  preview = {brain_tumor_mri.png}
}

@inproceedings{sajon_leukemia_2023,
  author = {Sajon, Tahsen Islam and Roy, Barsha and Faruk, Md. Farukuzzaman and Srizon, Azmain Yakin and Shuvo, Shakil Mahmud and Mamun, Md. Al and Sayeed, Abu and Hasan, S. M. Mahedy},
  title = {Attention Mechanism-Enhanced Deep CNN Architecture for Precise Multi-class Leukemia Classification},
  booktitle = {Proceedings of the <a href="https://confbim.com/">2nd International Conference on Big Data, IoT and Machine Learning</a>},
  year = {2023},
  pages = {349--361},
  note = {Fifth Author},
  url = {https://link.springer.com/chapter/10.1007/978-981-99-8937-9_24},
  abstract = {Leukemia is a life-threatening condition affecting people globally, making accurate diagnosis crucial for timely intervention. Consequently, researchers have been exploring automated methods to enable prompt action. The classification of leukemia into multiple subtypes according to WHO standards presents a unique challenge. Unlike binary classification, interclass features are highly similar, leading to misclassification. Ergo, we employ attention mechanisms to tackle this problem. Our proposed deep learning architecture combines transfer learning with attention mechanisms to classify subtypes of leukemia accurately. Using a publicly available dataset of blood cell images that adhered to WHO standards, we illustrate the potency of our approach. Our DenseNet201 with CBAM model achieves a remarkable 99.85% overall accuracy without resorting to data augmentation, surpassing previous methods on this dataset and attaining state-of-the-art results compared to other leukemia literature. To interpret the model's decision-making process and evaluate the efficacy of the attention mechanism in identifying discriminating features, we showcase GradCAM images and intermediate layer feature maps generated from our custom CNN. The proposed approach enhances leukemia classification accuracy and efficiency, providing clinical decision-making insights.},
  preview = {leukemia_classification.png}
}
